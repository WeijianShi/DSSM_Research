{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de63b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables and reference files needed \n",
    "TRAIN_DATA = '/Users/wesley/Desktop/4579_final/MRPC/train_data.csv'\n",
    "TEST_DATA = '/Users/wesley/Desktop/4579_final/MRPC/test_data.csv'\n",
    "\n",
    "VOCAB_FILE = './vocab_mrpc.txt'\n",
    "MODEL_FILE = './dssm_pr.pkl'\n",
    "\n",
    "N = 3\n",
    "SENTENCE_MAXLEN = 90\n",
    "CHAR_SIZE = 10041\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68bed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.0.0\n",
      "Uninstalling torch-2.0.0:\n",
      "  Would remove:\n",
      "    /Users/wesley/opt/anaconda3/bin/convert-caffe2-to-onnx\n",
      "    /Users/wesley/opt/anaconda3/bin/convert-onnx-to-caffe2\n",
      "    /Users/wesley/opt/anaconda3/bin/torchrun\n",
      "    /Users/wesley/opt/anaconda3/lib/python3.9/site-packages/functorch/*\n",
      "    /Users/wesley/opt/anaconda3/lib/python3.9/site-packages/torch-2.0.0.dist-info/*\n",
      "    /Users/wesley/opt/anaconda3/lib/python3.9/site-packages/torch/*\n",
      "    /Users/wesley/opt/anaconda3/lib/python3.9/site-packages/torchgen/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56eacc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create N_gram vector given a word\n",
    "def n_gram(word,n = N):\n",
    "    s = []\n",
    "    word = '#' + word + '#'\n",
    "    for i in range(len(word)-2):\n",
    "        s.append(word[i:i+n]) ## letter 3_gram \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ac25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create list of words in a sentence\n",
    "def lst_gram(lst,n = N):\n",
    "    s = []\n",
    "    for word in str(lst).lower().split():\n",
    "        s.extend(n_gram(word, n))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4055a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []  # initiate a vector to store letter n_grams\n",
    "file_path = './MRPC/'  # data set path\n",
    "files = ['train_data.csv','test_data.csv']  # training data & testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fd7170",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './MRPC/train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_s/18nnk14x26xd0hw8_j7261xw0000gn/T/ipykernel_80305/2944897142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# process train and test files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# line 0 is header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './MRPC/train_data.csv'"
     ]
    }
   ],
   "source": [
    "# process train and test files\n",
    "for file in files:\n",
    "    f = open(file_path + file,encoding = 'utf-8').readlines()\n",
    "    for i in range(1,len(f)):  # line 0 is header\n",
    "        s1,s2 = f[i][2:].strip('\\n').split('\\t')\n",
    "        # delete the first 3 character of each line, tab space, and line change space, parse into two sentences\n",
    "        vocab.extend(lst_gram(s1))\n",
    "        vocab.extend(lst_gram(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bb7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(vocab)\n",
    "vocab_list = ['[PAD]', '[UNK]']\n",
    "vocab_list.extend(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ce6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_FILE, 'w', encoding = 'utf-8') as f:\n",
    "    for slice in vocab_list:\n",
    "        f.write(slice)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9a05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab = open(VOCAB_FILE,encoding='utf-8').readlines()\n",
    "    slice2idx = {}\n",
    "    idx2slice = {}\n",
    "    count = 0\n",
    "    for char in vocab:\n",
    "        char = char.strip('\\n')\n",
    "        slice2idx[char] = cnt\n",
    "        idx2slice[count] = char\n",
    "        count += 1\n",
    "    return slice2idx, idx2slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c2e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(text, maxlen = SENTENCE_MAXLEN):\n",
    "    pad_text = []\n",
    "    for sentence in text:\n",
    "        pad_sentence = np.zeros(maxlen).astype('int64') #build a all zero vector that matches the shape of text\n",
    "        count = 0\n",
    "        for index in sentence:\n",
    "            pad_sentence[count] = index\n",
    "            count += 1\n",
    "            if count == maxlen:\n",
    "                break\n",
    "        pad_text.append(pad_sentence.tolist())\n",
    "    return pad_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a70a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_index(text_a,text_b):\n",
    "    slice2idx, idx2slice = load_vocab()\n",
    "    a_list,b_list = [],[]\n",
    "    \n",
    "    for a_sentence,b_sentence in zip(text_a,text_b): # for each line (parsed into two sentences)in the file\n",
    "        a,b = [],[]\n",
    "        for slice in lst_gram(a_sentence): # for each slice of the first sentence in each line\n",
    "            if slice in slice2idx.keys():\n",
    "                a.append(slice2idx[slice])\n",
    "            else:\n",
    "                a.append(1)\n",
    "                \n",
    "        for slice in lst_gram(b_sentence): # for each slice of the second sentence in each line\n",
    "            if slice in slice2idx.keys():\n",
    "                b.append(slice2idx[slice])\n",
    "            else:\n",
    "                b.append(1)\n",
    "                \n",
    "        a_list.append(a)\n",
    "        b_list.append(b)\n",
    "        \n",
    "    a_list = padding(a_list)\n",
    "    b_list = padding(b_list)\n",
    "        \n",
    "    return a_list,b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1efbe722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_char_data(filename):\n",
    "    df = pd.read_csv(filename, sep = '\\t')\n",
    "    text_a = df['#1 string'].values\n",
    "    text_b = df['#2 string'].values\n",
    "    label = df['quality'].values\n",
    "    a_index,b_index = char_index(text_a,text_b)\n",
    "    return np.array(a_index),np.array(b_index),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0ed57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
